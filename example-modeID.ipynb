{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1a25ba6",
   "metadata": {},
   "source": [
    "# Example of using The Module Formerly Known as KDE and Asy_peakbag\n",
    "This is a notebook showing a rough draft of the replacement module for KDE and Asy_peakbag, which is currently WIP on the PBjam-dev branch.\n",
    "\n",
    "We'll start by importing the modeID sampler (Should the name be changed?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "from pbjam.modeID import modeIDsampler\n",
    "from pbjam import IO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0c3be",
   "metadata": {},
   "source": [
    "We'll also use the IO module to load the PSD. This is a new module that contains all I/O related matters, saving/loading/downloading etc. The largest part of this is wrapping the various lightkurve steps to getting a PSD. However it now also stores lightkurve search results so that cached data are loaded more quickly. \n",
    "\n",
    "The IO.psd class also normalizes the PSD consistently (Parseval). This is important now that we're working on the PSD and not the SNR spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad76671",
   "metadata": {},
   "outputs": [],
   "source": [
    "psd = IO.psd('KIC10963065', lk_kwargs={'exptime': 60, 'mission':'Kepler', 'author':'Kepler'})\n",
    "\n",
    "psd()\n",
    "\n",
    "f = psd.freq[::2]\n",
    "\n",
    "s = psd.powerdensity[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526e07e6",
   "metadata": {},
   "source": [
    "## Priors\n",
    "With the update to PBjam most of the parameters are included in the dimensionality reduction (DR). The latent parameter priors are constructed by the DR module, using the the prior sample of the model parameters. For the g-mode parameters the prior sample is not even remotely complete however. To get around this we can supply the modeID class with a list of priors for these parameters. They will then automatically be removed from the list of parameters included in the DR.\n",
    "\n",
    "These priors are all in the form of class instances of distributions, similar to the scipy.stats classes. However, here we're using a set of custom classes that have been jaxed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535436e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pbjam.distributions as dist\n",
    "import numpy as np\n",
    "\n",
    "addPriors = { \n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af665d38",
   "metadata": {},
   "source": [
    "These location and scale parameters for the priros are suitable for a cool MS star. The g-mode parameters are of course non-sense, and all fits to MS stars just recover this prior since the p-modes can't feel any of the g-modes yet. For lower numax targets, the prior parameters must be chosen by hand (see the bottom of this notebook).\n",
    "\n",
    "The pbjam.distributions module contains a short list of 'standard' distributions,these include normal, uniform, beta and a few others.. The jax.scipy.stats module could replace some of these functions, but they are far from complete (many don't have a ppf method for example). If you want to construct your own distribution you can do so, although it must have a ppf method which is required by Dynesty.\n",
    "\n",
    "If you wish to add a distribution as a prior, you'll need to use the correct keyword as everything is now keyword-based rather than index-based. You can view a list of all keywords for the model parameters, whether they are treated in log10 or not and whether they are included in the DR by default in the `modeIDsampler.variables` attribute of the sampler class. You can edit this before initializing the class below in case you want to set a parameter to log10, but you won't need to edit things if you're just adding priors. This will be handled automatically. **Ideally you won't need to edit this at all**\n",
    "\n",
    "Apart from those listed above, the shot noise and the very lowest frequency harvey law have hardcoded priors. It's safe to ignore these for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4f19b",
   "metadata": {},
   "source": [
    "## Additional observational parameters\n",
    "As in the earlier PBjam versions we use additional observational parameters. $\\nu_{max}$, $\\Delta\\nu$ and $T_{eff}$ are used to select the prior sub-sample to construct the covariance matrix for DR, while $T_{eff}$ and $G_{bp}-G_{rp}$ are used in the actual sampling as additional log-likelihood terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e714583",
   "metadata": {},
   "outputs": [],
   "source": [
    "addObs = {'numax': (2204, 100),\n",
    "          'dnu'  : (103.2, 0.54),\n",
    "          'teff' : (6140, 77),\n",
    "          'bp_rp': (0.70026, 0.05),\n",
    "          } "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428b383",
   "metadata": {},
   "source": [
    "## Initialize the sampler\n",
    "We can now initialize the sampler class. This requires specifying the number of orders `N_p` (previously `norders`) to include in the model. In addition we also set the number of targets (`N_pca`) to draw from the prior sample to construct covariance matrix for DR, and the number of dimensions (`PCAdims`) to use in the sampling.  \n",
    "\n",
    "TODO: add in a check that `PCAdims` + non-PCA dims <= all model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac16603",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_p = 7\n",
    "\n",
    "N_pca = 100\n",
    "\n",
    "PCAdims = 8\n",
    " \n",
    "M = modeIDsampler(f, s, addObs, addPriors, N_p=N_p, Npca=N_pca, PCAdims=PCAdims, priorpath=IO.get_priorpath())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8b3ee",
   "metadata": {},
   "source": [
    "## Plot the prior\n",
    "It's always good to check that your prior is reasonable, so lets plot some samples from the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d993fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import astropy.convolution as conv\n",
    "import numpy as np\n",
    "\n",
    "fac = max([1, 0.5 / (M.f[1] - M.f[0])])\n",
    "kernel = conv.Gaussian1DKernel(stddev=np.array(fac))\n",
    "smoo = conv.convolve(M.s, kernel)\n",
    "\n",
    "N = 20\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "ax.loglog(M.f, smoo)\n",
    "\n",
    "for i in range(N):\n",
    "    u = np.random.uniform(0, 1, size=M.ndims)\n",
    "\n",
    "    theta = M.ptform(u)\n",
    "    \n",
    "    theta_u = M.unpackParams(theta)\n",
    "     \n",
    "    m = M.model(theta_u, M.f)\n",
    "    \n",
    "    ax.loglog(M.f, m, alpha = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0d10ca",
   "metadata": {},
   "source": [
    "## Run Dynesty\n",
    "Time to run! You can optionally run with the dynamic sampler enabled, but it probably won't do you much good and take an order of magnitude longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d1234",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler, samples = M(nlive=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4411e6ff",
   "metadata": {},
   "source": [
    "## Plot model samples\n",
    "Time to plot some of the posterior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90849fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "ax.plot(M.f, smoo)\n",
    " \n",
    "N = 20\n",
    "for i in np.random.randint(0, samples.shape[0], size=N):\n",
    "    \n",
    "    theta = samples[i, :]\n",
    "\n",
    "    theta_u = M.unpackParams(theta)\n",
    "     \n",
    "    m = M.model(theta_u, M.f)\n",
    "    \n",
    "    ax.plot(M.f, m, color='C3', alpha=0.25)\n",
    "    \n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "ax.plot(M.f, smoo)\n",
    " \n",
    "for i in np.random.randint(0, samples.shape[0], size=N):\n",
    "    \n",
    "    theta = samples[i, :]\n",
    "\n",
    "    theta_u = M.unpackParams(theta)\n",
    "     \n",
    "    m = M.model(theta_u, M.f)\n",
    "    \n",
    "    ax.plot(M.f, m, color='C3', alpha=0.25)\n",
    "ax.set_xlim(addObs['numax'][0]-7*addObs['dnu'][0], addObs['numax'][0]+7*addObs['dnu'][0])\n",
    "ax.set_ylim(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd558d",
   "metadata": {},
   "source": [
    "## Corner plot\n",
    "We can make a nice corner plot too. However, the samples from the priors contain a mix of latent parameters and model parameters, some in log10, others not. To get everything in terms of linear model parameters we'll need to transform the raw samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2269e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_u = M.unpackSamples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "corner.corner(samples_u);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bdf3a1",
   "metadata": {},
   "source": [
    "Or any subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9bb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner({k : samples_u[k] for k in ['dnu', 'numax', 'd01', 'DPi0', 'eps_g', 'p_L0', 'p_D0']});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d82f6b",
   "metadata": {},
   "source": [
    "## Parameters for other stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KIC8524425\n",
    "# obs = {'teff': (5413, 80),\n",
    "#        'bp_rp': (0.9026, 0.05), \n",
    "#        'numax': (1084.7, 10.0),\n",
    "#        'dnu': (59.62, 0.21)}\n",
    "\n",
    "\n",
    "##############\n",
    "# OUT OF DATE - NEED TO LOG ALL THESE VALUES\n",
    "##############\n",
    "# # These parameters are suitable for a cool subgiant at ~1000 muHz.\n",
    "# addPriors = {'p_L0': dist.normal(loc=0.024, scale=0.1 * 0.024), # First coefficient of coupling strength matrix poly.\n",
    "#              'p_D0': dist.normal(loc=0.0036, scale=0.1 * 0.0036), # First coefficient of overlap integral matrix poly.\n",
    "#              'DPi0': dist.normal(loc=0.000476, scale=0.1 * 0.000476), # period spacing in 1/muHz (mega seconds)\n",
    "#              'eps_g': dist.normal(loc=0.81, scale=0.1 *0.81), # epsilon for g-modes\n",
    "#              'alpha_g': dist.normal(loc=0.02, scale=0.002), # curvature for g-modes\n",
    "#              'd01': dist.normal(loc=26, scale=2.6), # absolute l=0,1 spacing in muHz.\n",
    "#              'nurot_c': dist.uniform(loc=0.01, scale=.05) # Core rotation rate in log(muHz)\n",
    "#             }\n",
    "\n",
    "\n",
    " \n",
    "       \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
